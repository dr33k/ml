# -*- coding: utf-8 -*-
"""plagiarism_detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdVV33QG5eeD6_J3mjj2_IZeVoUQi389

Import Dependencies
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
from google.colab import drive
drive.mount('/content/gdrive')

"""Google Drive file paths"""

path = '/content/gdrive/My Drive/datasets/{}.parquet'
file_names = ['a']
dataframes = list()

"""Implement Preprocess function"""

def preprocess_data(sample_size):

  #Compile a list of dataframes
  for name in file_names:
    dataframes.append(pd.read_parquet(path.format(name)))

  #Vertically concatenate dataframes
  df = pd.concat(dataframes, axis=0)

  #Drop rows where text data is null
  df = df.dropna(subset=['text']).reset_index(drop=True)

  #Sample
  df = df.sample(sample_size)

  return df

# Preprocess data and sample 100 from it
preprocessed_data = preprocess_data(100)

#Shape
print("Shape:", preprocessed_data.shape)

#Head
preprocessed_data.head()

"""Install torch and transformers"""

pip -q install torch transformers

"""Load Bert Model"""

import torch
from keras.preprocessing.sequence import pad_sequences
from transformers import BertTokenizer, AutoModelForSequenceClassification

model_path = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)
model = AutoModelForSequenceClassification.from_pretrained(model_path, output_attentions=False, output_hidden_states=True)

"""Create Vector from Text"""

def create_vector(text, MAX_LEN=510):

  #Generate Inmput ids for text
  input_ids = tokenizer.encode(text, add_special_tokens=True, max_length=MAX_LEN)

  #Pad sequences with more than the Maximum length of tokens
  results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

  #Remove the outer list
  input_ids = results[0]

  #Create attention mask
  attention_mask = [int(i>0) for i in input_ids]

  #Convert to tensors
  input_ids = torch.tensor(input_ids)
  attention_mask = torch.tensor(attention_mask)

  #Add an extra redundant dimension for the batch
  input_ids=input_ids.unsqueeze(0)
  attention_mask=attention_mask.unsqueeze(0)

  #Put the model n evaluation mode, meaning a feed-forward operation
  model.eval()

  #Pass the text to BERT to get the vector embedding and collect all hidden states from the 12 encoders
  with torch.no_grad():
    logits, encoded_layers = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        return_dict=False,
        token_type_ids=None
    )

  #Extract the embedding
  vector = encoded_layers[-1][0][0]

  #Detach to cpu and convert to numpy array
  vector = vector.detach().cpu().numpy()

  return (vector)

"""Create Vector index"""

def create_vector_index(df):
  vectors = []

  #Get overall text data
  source_data = df.text.values
  print(source_data)

  for text in tqdm(source_data):
    vector = create_vector(text)
    vectors.append(vector)

  #Create a new column called vectors and reshape the array
  df['vectors'] = vectors
  df['vectors'] = df['vectors'].apply(lambda v: np.array(v).reshape(1, -1))

  return df

vector_index = create_vector_index(preprocessed_data)

"""Analysis using cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity

def prepare_for_cosine_similarity(text):
  vector = create_vector(text)
  vector = np.array(vector).reshape(1, -1);
  return vector

def is_plagiarism(similarity_score, plagiarism_threshold):
  return similarity_score > plagiarism_threshold

def analyse_plagiarism(query_text, plagiarism_threshold=0.8):
  query_vector = prepare_for_cosine_similarity(query_text)

  # Create a similarity column
  vector_index['similarity'] = vector_index['vectors'].apply(lambda x: cosine_similarity(query_vector, x))
  vector_index['similarity'] = vector_index['similarity'].apply(lambda x: x[0][0])

  #Get first 3 most similar articles
  similar_articles = vector_index.sort_values(by='similarity', ascending = False)[1:4]

  #Format columns
  formatted_similar_articles = similar_articles[['id', 'title', 'similarity']].reset_index(drop=True)

  #Similrity_score
  similarity_score = formatted_similar_articles.iloc[0]['similarity']

  #Most similar article
  most_similar_article_id = formatted_similar_articles.iloc[0]['id']
  most_similar_article_title = formatted_similar_articles.iloc[0]['title']

  plagiarism_result = {
      'similarity_score': similarity_score,
      'most_similar_article_id': most_similar_article_id,
      'most_similar_article_title': most_similar_article_title,
      'is_plagiarism': is_plagiarism(similarity_score, plagiarism_threshold),
      'article_submitted': query_text
  }

  return plagiarism_result

"""Testing"""

query_text = input('Enter submission to check for plagiarism:')
result = analyse_plagiarism(query_text)
print(result)